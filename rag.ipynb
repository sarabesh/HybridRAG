{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOKd6_IfWVQY"
      },
      "source": [
        "*Notebook for comparing RAG based on vector indexes vs Knowledge graphs*\n",
        "\n",
        "Tools and services used:\n",
        "- Databases: Neo4j Desktop(local neo4j server) to store vectors for vector RAG and  Neo4j Aura DB instance store knowledge graphs.\n",
        "- LLM-graph-builder : https://llm-graph-builder.neo4jlabs.com/, tool used to convert pdf documents to knowledge graphs.\n",
        "- LLMs: Gemini 1.5 for knowledge graph generation, Gemini-pro for cypher query mapping and llama3.2:8b for RAG.\n",
        "- Ollama: Serving Llama3.2, for embedding generations and prompting in RAG.\n",
        "- RAG orchestration: Langchain components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frc0GVv1WVQd"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain.embeddings.ollama import OllamaEmbeddings\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain.graphs import Neo4jGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UFDoJoFWVQf"
      },
      "source": [
        "*Datasets:*\n",
        "\n",
        "We will be using wiki documents on Formula1 championship for performing RAG, and comparing by performing set of queries on the documents with RAGs. The documents are stored in the \\data folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_TYyrtAWVQf"
      },
      "outputs": [],
      "source": [
        "#drivers to connect to neo4j-graph running locally. We will be using this for vector based RAG.\n",
        "url = \"bolt://localhost:7689\"\n",
        "username =\"neo4j\"\n",
        "password = \"password\"\n",
        "\n",
        "graph = Neo4jGraph(\n",
        "    url=url,\n",
        "    username=username,\n",
        "    password=password\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW68E25-WVQh"
      },
      "source": [
        "**Vector RAG**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2XcBoT7WVQh"
      },
      "source": [
        "We split documents into chunks and embed them using llama3.2 Ollama embeddings, to be used for vector RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owlhh3zYWVQi"
      },
      "outputs": [],
      "source": [
        "# Define the folder containing PDF files\n",
        "folder_path = \"./data\"\n",
        "\n",
        "# Initialize an embedding model\n",
        "embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
        "\n",
        "# Load all PDF files from the folder\n",
        "pdf_documents = []\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith(\".pdf\"):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        pdf_documents.extend(loader.load())\n",
        "\n",
        "# Split the text into chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(pdf_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOns7IZyWVQi"
      },
      "outputs": [],
      "source": [
        "# Connect to Neo4j and save embeddings\n",
        "db = Neo4jVector.from_documents(\n",
        "    docs,\n",
        "    embedding=embeddings,\n",
        "    graph = graph\n",
        ")\n",
        "\n",
        "print(\"PDF documents have been successfully processed and saved to Neo4j!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZEidJ8nWVQj"
      },
      "outputs": [],
      "source": [
        "#Creates a retriever to fetch the top 5 most similar vectors from the database using similarity search.\n",
        "vector_retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73SKiGQOWVQj"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms.ollama import Ollama\n",
        "\n",
        "llm = Ollama(model=\"llama3.2\")\n",
        "\n",
        "#Sets up a Retrieval-Augmented QA pipeline where the LLM answers queries using relevant information retrieved from the vector database.\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vector_retriever,\n",
        "    return_source_documents=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inp2x0SrWVQk"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVOTmMb-WVQk"
      },
      "source": [
        "We’ve implemented the flow using LangChain components, where we’re performing RAG with vector-based search. Here, we’re using Llama3.2:8b as the LLM. It takes the input query, retrieves context by finding similar vector embeddings, and then combines the context with the query to generate a response. Let's try some queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-SlovBbWVQz"
      },
      "outputs": [],
      "source": [
        "rag_chain.invoke(\"who is the best?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_Z7lDSsWVQ0"
      },
      "outputs": [],
      "source": [
        "rag_chain.invoke(\"Who has most wins in 2024?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcp8gmVEWVQ1"
      },
      "outputs": [],
      "source": [
        "rag_chain.invoke(\"Who will win in 2024?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVnBFwD-WVQ1"
      },
      "outputs": [],
      "source": [
        "rag_chain.invoke(\"Who came second in 2023?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4ansimCWVQ2"
      },
      "outputs": [],
      "source": [
        "rag_chain.invoke(\"Who is the best driver and best teams across the championships 2022-24\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaX8t-4PWVQ2"
      },
      "source": [
        "**Graph RAG**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxH20Kl7WVQ3"
      },
      "source": [
        "*Knowledge Graph generation and setup:*\n",
        "\n",
        "To ensure a fair comparison for RAG, I decided to use the same source of data for both setups. Initially, I planned to create Knowledge Graphs from the documents using LLMGraphTransformer, but I faced issues getting it to work with the Gemini LLM, the only free-supported option. As a workaround, I opted to use Neo4j's *llm-graph-builder* tool, available at *https://llm-graph-builder.neo4jlabs.com/*, which worked effectively for my needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGPFACZYWVQ3"
      },
      "source": [
        "The Neo4j LLM Knowledge Graph Builder is an online tool that transforms unstructured text into a knowledge graph, offering a seamless and intuitive text-to-graph experience.\n",
        "\n",
        "It leverages multiple ML models (LLMs), including OpenAI, Gemini, Llama3, Diffbot, Claude, and Qwen, to process various sources like PDFs, documents, images, web pages, and YouTube video transcripts. The tool extracts relevant information to create two main graph structures:\n",
        "\n",
        "A lexical graph of documents and chunks (with embeddings)\n",
        "An entity graph with nodes representing entities and their relationships\n",
        "Both of these graphs are stored in your Neo4j database.\n",
        "\n",
        "You can customize the extraction schema and perform clean-up operations after the extraction. Once the data is organized, you can utilize various RAG approaches (GraphRAG, Vector, Text2Cypher) to query your data, gaining insights based on the extracted information and observing how it contributes to constructing accurate answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcSZMsirWVQ4"
      },
      "source": [
        "### Steps to Generate a Knowledge Graph using the llm-graph-builder Tool  \n",
        "\n",
        "1. **Upload Documents**  \n",
        "   - Upload the documents containing the text data to the **llm-graph-builder** tool.\n",
        "\n",
        "2. **Select the LLM Model**  \n",
        "   - Choose the desired **LLM model** to perform entity extraction and graph creation.\n",
        "\n",
        "3. **Configure Extraction Parameters**  \n",
        "   - Set up any necessary **extraction parameters** or options to customize the entity and relationship extraction process.\n",
        "\n",
        "4. **Click \"Generate Graph\"**  \n",
        "   - Click the **\"Generate graph\"** button to start the graph creation process.\n",
        "\n",
        "5. **LLM Processes the Data**  \n",
        "   - The selected LLM analyzes the text, **extracting entities** (e.g., names, locations, concepts) and identifying **relationships** among them (e.g., \"Person A works at Company B\").\n",
        "\n",
        "6. **Construct the Knowledge Graph**  \n",
        "   - The tool organizes the extracted entities and relationships into a **graph structure**, creating nodes (entities) and edges (relationships).\n",
        "\n",
        "7. **Visualize the Graph**  \n",
        "   - Finally, the tool displays the Knowledge Graph in an **interactive interface**, showing a clear, visual layout of interconnected entities and their relationships.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3tK9XQCWVQ4"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xHTS5kiWVQ4"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy89XC0BWVQ4"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRBZDLx_WVQ5"
      },
      "source": [
        "It resulted in 5,364 nodes and 37,055 relationships, effectively capturing a vast amount of entities and their connections within the knowledge graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf8SjmYkWVQ5"
      },
      "outputs": [],
      "source": [
        "#using neo4j-aura instance, since we get graph from the tool, and it create large number of nodes.\n",
        "url = \"neo4j+s://66c55e03.databases.neo4j.io:7687\"\n",
        "username =\"neo4j\"\n",
        "password = \"QQOiQX0Hr983z3IuQtdL7QETKdyWb41ZIqMp2j0FF_Q\"\n",
        "\n",
        "#the documents are converted to knowledge graphs and added to this server.\n",
        "graph2 = Neo4jGraph(\n",
        "    url=url,\n",
        "    username=username,\n",
        "    password=password\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th-NeRgaWVQ5"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5ofqUtvWVQ5"
      },
      "source": [
        "This setup is for GraphRAG, also known as Hybrid RAG, which combines context from both unstructured text and knowledge graphs, effectively integrating information from multiple sources to enhance data retrieval and reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGG471H8WVQ5"
      },
      "source": [
        "*Creating retriever for unstructured data.*\n",
        "\n",
        "This code sets up a hybrid search index in Neo4j by combining graph traversal and vector similarity search. It uses the Llama3.2 model from Ollama to generate text embeddings, which are stored in the graph's nodes labeled as 'Chunk' under the embedding property. The Neo4jVector.from_existing_graph function indexes an existing graph, enabling efficient hybrid search queries that combine relationships in the graph structure with semantic similarity search on text embeddings, facilitating better retrieval and reasoning over the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT3AZA5kWVQ6"
      },
      "outputs": [],
      "source": [
        "#creating vector indices to be used by RAG . Creates embeddings using Ollama llama llm, and stores them in neo4j\n",
        "import os\n",
        "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain.embeddings.ollama import OllamaEmbeddings\n",
        "\n",
        "\n",
        "vector_index = Neo4jVector.from_existing_graph(\n",
        "    OllamaEmbeddings( model=\"llama3.2\",),\n",
        "    graph = graph2,\n",
        "    search_type=\"hybrid\",\n",
        "    node_label='Chunk',\n",
        "    text_node_properties=[\"text\"],\n",
        "    embedding_node_property='embedding',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1VwLiV3WVQ6"
      },
      "outputs": [],
      "source": [
        "#returning similar vector (cosine similarity)\n",
        "response = vector_index.similarity_search(\n",
        "    \"who is max?\"\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toMLBNLwWVQ7"
      },
      "outputs": [],
      "source": [
        "vector_keyword_retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSimzsOWWVQ7"
      },
      "outputs": [],
      "source": [
        "#return llm's response, on query+rag_context node vector input to LLM\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOllama\n",
        "import ollama_function\n",
        "\n",
        "\n",
        "vector_keyword_chain= RetrievalQA.from_chain_type(\n",
        "    llm=ollama_function.OllamaFunctions(model=\"llama3.2\"),\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_keyword_retriever,\n",
        "    return_source_documents=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4QJ0oh8WVQ8"
      },
      "source": [
        "Now, we have half the rag, which get unstructured context using both vector and text indexes. Let's try running some queries with it alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIoSyS8bWVQ8"
      },
      "outputs": [],
      "source": [
        "vector_keyword_chain.invoke(\"who is the best driver?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx7PfXt3WVQ8"
      },
      "outputs": [],
      "source": [
        "vector_keyword_chain.invoke(\"Who has most wins in 2024?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A31P74-8WVQ9"
      },
      "outputs": [],
      "source": [
        "vector_keyword_chain.invoke(\"Who will win in 2024?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p7OW8t2WVQ9"
      },
      "outputs": [],
      "source": [
        "vector_keyword_chain.invoke(\"Who came second in 2023?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HII9GA7WVQ9"
      },
      "source": [
        "*Creating retriever for structured data.*\n",
        "\n",
        "Configuring a graph retrieval setup is more complex but provides greater flexibility. In this example, we use a full-text index to find relevant nodes and then retrieve their direct neighborhood. The graph retriever begins by identifying pertinent entities in the input, such as people, organizations, and locations. To accomplish this, we employ LCEL along with the newly introduced with_structured_output method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZilRAzGkWVQ-"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbwxwS0VWVQ_"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from typing import Tuple, List, Optional\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True,GOOGLE_API_KEY=GOOGLE_API_KEY)\n",
        "graph2.query(\n",
        "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
        "\n",
        "# Extract entities from text\n",
        "class Entities(BaseModel):\n",
        "    \"\"\"Identifying information about entities.\"\"\"\n",
        "\n",
        "    names: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"All the person, organization, or business entities that \"\n",
        "        \"appear in the text\",\n",
        "    )\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are extracting organization and person entities from the text.\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Use the given format to extract information from the following \"\n",
        "            \"input: {question}\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "entity_chain = prompt | llm.with_structured_output(Entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HQt5JTvWVQ_"
      },
      "outputs": [],
      "source": [
        "entity_chain.invoke({\"question\": \"Who is max verstappen?\"}).names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF9qP2woWVQ_"
      },
      "source": [
        "We can see that it is able to get the entities from the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hGA7V8FWVQ_"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
        "graph2.query(\n",
        "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
        "\n",
        "def generate_full_text_query(input: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a full-text search query for a given input string.\n",
        "\n",
        "    This function constructs a query string suitable for a full-text\n",
        "    search. It processes the input string by splitting it into words and\n",
        "    appending a similarity threshold (~2 changed characters) to each\n",
        "    word, then combines them using the AND operator. Useful for mapping\n",
        "    entities from user questions to database values, and allows for some\n",
        "    misspelings.\n",
        "    \"\"\"\n",
        "    full_text_query = \"\"\n",
        "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
        "    for word in words[:-1]:\n",
        "        full_text_query += f\" {word}~2 AND\"\n",
        "    full_text_query += f\" {words[-1]}~2\"\n",
        "    return full_text_query.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-0a9h-KWVQ_"
      },
      "outputs": [],
      "source": [
        "# Fulltext index query\n",
        "def structured_retriever(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Collects the neighborhood of entities mentioned\n",
        "    in the question\n",
        "    \"\"\"\n",
        "    result = \"\"\n",
        "    entities = entity_chain.invoke({\"question\": question})\n",
        "    print(entities.names)\n",
        "    for entity in entities.names:\n",
        "        response = graph2.query(\n",
        "            \"\"\"CALL db.index.fulltext.queryNodes('entities', $query,\n",
        "            {limit:2})\n",
        "            YIELD node,score\n",
        "            CALL {\n",
        "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
        "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS\n",
        "              output\n",
        "              UNION\n",
        "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
        "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS\n",
        "              output\n",
        "            }\n",
        "            RETURN output LIMIT 50\n",
        "            \"\"\",\n",
        "            {\"query\": generate_full_text_query(entity)},\n",
        "        )\n",
        "        # for el in response:\n",
        "        #     print(el['output'])\n",
        "        result += \"\\n\".join([el['output'] for el in response if el['output'] is not None])\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAiSag3NWVRA"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(structured_retriever(\"Who is Max Verstappen?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFFFb5HEWVRA"
      },
      "source": [
        "We can see the graph retreiver is able to retrieve the related nodes from the graph. Now we can combine these outputs with vector retriever output, as part of a rag chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWTwSvLGWVRA"
      },
      "outputs": [],
      "source": [
        "repsonse = vector_index.similarity_search(\"who is verstappen?\")\n",
        "print(\"received data .....\")\n",
        "\n",
        "for el in response:\n",
        "    print(el.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ykZNa0rWVRA"
      },
      "outputs": [],
      "source": [
        "#This retriever combines both vector and graph retrievers for same question.\n",
        "def retriever(question: str):\n",
        "    print(f\"Search query: {question}\")\n",
        "    structured_data = structured_retriever(question)\n",
        "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
        "    final_data = f\"\"\"Structured data:\n",
        "{structured_data}\n",
        "Unstructured data:\n",
        "{\"#Document \". join(unstructured_data)}\n",
        "    \"\"\"\n",
        "    print(final_data)\n",
        "    return final_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op9sSnwRWVRA"
      },
      "outputs": [],
      "source": [
        "# llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True)\n",
        "llm = Ollama(model=\"llama3.2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvDv5M6UWVRB"
      },
      "source": [
        "Here’s the RAG chain setup: it takes an input query, retrieves a combined context from both vector embeddings and graph data, and then passes this along with the query to the LLM. The LLM generates an output, which is subsequently refined and printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mmdwiOeWVRB"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import (\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "from langchain_core.output_parsers.string import StrOutputParser\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"context\": retriever,\n",
        "            \"question\": RunnablePassthrough(),\n",
        "        }\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh6BrYNuWVRB"
      },
      "outputs": [],
      "source": [
        "response = chain.invoke(input=\"Who is max verstappen?\")\n",
        "print(\"Final Answer\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fsZyfo8WVRB"
      },
      "source": [
        "The output explains the hybrid retrieval process for RAG by describing how the system combines vector and graph search approaches. It first retrieves context by searching through the vector index using embedding vectors and keywords. Then, it searches the knowledge graph to find related entities and their connections. These two contexts—vector-based and graph-based—are combined and fed into the LLM. The LLM then processes the combined information to generate the final, accurate answer. This hybrid approach leverages both unstructured data (through embeddings and keyword search) and structured data (through graph connections) to enhance the retrieval process in RAG applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVhStw0QWVRC"
      },
      "outputs": [],
      "source": [
        "response = chain.invoke(input=\"Who will win 2024 f1 championship?\")\n",
        "print(\"Final Answer\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOU_qCblWVRC"
      },
      "outputs": [],
      "source": [
        "response = chain.invoke(input=\"Who won 2023 f1 championship?\")\n",
        "print(\"Final Answer\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXFt8PrYWVRC"
      },
      "outputs": [],
      "source": [
        "response = chain.invoke(input=\"Who came second in 2023 f1 championship?\")\n",
        "print(\"Final Answer\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oxFghawWVRC"
      },
      "outputs": [],
      "source": [
        "response = chain.invoke(input=\"Which team won in 2023 f1 championship\")\n",
        "print(\"Final Answer\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbsKUh26WVRC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}